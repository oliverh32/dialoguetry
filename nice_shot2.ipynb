{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli as bern\n",
    "import heapq\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab\n",
    "model_dir = 'gpt_vocab'\n",
    "enc = encoder.get_encoder(model_dir)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# define parameters\n",
    "class HParams():\n",
    "    n_vocab = 50257\n",
    "    n_embed = 768\n",
    "    start_token = 50256\n",
    "    batch_size = 32\n",
    "    n_tuples = 2000\n",
    "    device = device\n",
    " \n",
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hparams, input_dim = 768, hid_dim = 256):\n",
    "        super(Attn,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        # Number of triples selected\n",
    "        self.hparams = hparams\n",
    "        self.k = hparams.n_tuples\n",
    "        self.attn = nn.Sequential(nn.Linear(input_dim*2,hid_dim), nn.Linear(hid_dim,1))\n",
    "\n",
    "    def forward(self, h_k, h_c):\n",
    "        #h_k = [batch_size, k, hid_dim]\n",
    "        #h_c = [batch_size, hid_dim]\n",
    "\n",
    "        h_c =  h_c.unsqueeze(1) #[batch_size, 1, hid_dim]\n",
    "        h_c = torch.cat([h_c]*self.k,dim=1)    #[batch_size, k, hid_dim]\n",
    "        h_k = h_k.unsqueeze(0)\n",
    "        h_k = torch.cat([h_k]*self.hparams.batch_size,dim=0)    #[batch_size, k, hid_dim]\n",
    "        h_comb = torch.cat((h_k,h_c),dim=2)   #[batch_size, k, hid_dim*2]\n",
    "\n",
    "        attn_logits = self.attn(h_comb).squeeze(2)  #[batch_size,k]\n",
    "        attn_weight = F.softmax(attn_logits).unsqueeze(1)\n",
    "        h_k_comb = torch.bmm(attn_weight,h_k).squeeze(1)    #[batch_size,hid_dim]\n",
    "        return h_k_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self,embedding_size = 256, num_units = 768, vocab_size = 50257, dropout_p = 0.1, num_layers = 2):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_units = num_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size) # !\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size=num_units,num_layers=num_layers, batch_first = True)\n",
    "\n",
    "        self.Linear = nn.Linear(num_units,vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        #input = [batch_size]\n",
    "        #hidden = [batch_size, 2, hid_dim]\n",
    "        #cell = [batch_size, 2, hid_dim]\n",
    "\n",
    "        input = input.unsqueeze(1)  #[batch_size, 1]\n",
    "        # print(\"Input shape:\",input.shape)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(input)) #[batch_size, 1, emb_dim]\n",
    "        # print(\"Embedding shape:\",embedding.shape)\n",
    "        # print(\"hidden shape\",hidden.shape)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell)) \n",
    "\n",
    "        #output = [batch_size, 1, hid_dim]\n",
    "        #hidden = [2, batch_size hid_dim]\n",
    "        #cell  = [2, batch_size, hid_dim]\n",
    "\n",
    "        logits = self.Linear(output)  #[batch_size, vocab_size]\n",
    "\n",
    "        return logits, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hparams, embedding_size = 256, num_units = 768, vocab_size = 50257, dropout_p = 0.1, seq_len = 100, batch_size = 32, teacher_forcing_ratio = 0.5):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.LSTM = DecoderLSTM(embedding_size, num_units,vocab_size,dropout_p)\n",
    "        self.start_token = hparams.start_token\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.hparams = hparams\n",
    "\n",
    "\n",
    "    def forward(self, trg, h_c, h_k):\n",
    "        #h_c = [batch_size, hid_dim]\n",
    "        #h_k = [batch_size, hid_dim]\n",
    "        #trg = [batch_size, seq_len]\n",
    "\n",
    "        input = trg[:,0]\n",
    "        hidden = torch.stack((h_c,h_k),dim=0)\n",
    "        cell = torch.zeros(2, self.batch_size,self.LSTM.num_units).to(self.hparams.device)\n",
    "        outputs = torch.zeros(self.batch_size, 1, self.LSTM.vocab_size).to(self.hparams.device)\n",
    "        # print('trg_shape: ', trg.shape[1])\n",
    "\n",
    "        for t in range(1, trg.shape[1]):\n",
    "            output, hidden, cell = self.LSTM.forward(input,hidden,cell)\n",
    "            # print(output.shape)\n",
    "\n",
    "            # outputs = torch.cat([outputs,output],dim=1)\n",
    "            outputs = torch.cat([outputs,output],dim=1)\n",
    "            # print('outputs: ', outputs[:,1:].shape)\n",
    "\n",
    "            top1 = output.squeeze(1).argmax(1)\n",
    "            # print(\"top1_shape\",top1.shape)\n",
    "\n",
    "            replace = np.random.random() < self.teacher_forcing_ratio\n",
    "\n",
    "            input = trg[:,t] if replace else top1\n",
    "            # print(input.shape)\n",
    "\n",
    "        # print('outputs: ', outputs[:,1:].shape)\n",
    "        return outputs[:,1:]\n",
    "\n",
    "\n",
    "    def decode(self, h_c, h_k, seq_len):\n",
    "      with torch.no_grad():\n",
    "        input = torch.LongTensor([self.hparams.start_token]*self.batch_size).to(self.hparams.device)\n",
    "        hidden = torch.stack((h_c,h_k),dim=0)\n",
    "        cell = torch.zeros(2,self.batch_size,self.LSTM.num_units).to(self.hparams.device)\n",
    "        tokens = None\n",
    "        # tokens = torch.LongTensor(np.zeros([self.batch_size,1]))\n",
    "        for t in range(0, seq_len-1):\n",
    "            output, hidden, cell = self.LSTM.forward(input,hidden,cell)\n",
    "            input = output.squeeze(1).argmax(1)\n",
    "            tokens = input.unsqueeze(1) if tokens == None else torch.cat([tokens,input.unsqueeze(1)],dim=1)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hashmodel(nn.Module):\n",
    "    def __init__(self,hparams):\n",
    "        super(Hashmodel,self).__init__()\n",
    "        self.attn = Attn(hparams)\n",
    "        self.decoder = Decoder(hparams)\n",
    "        self.hparams = hparams\n",
    "        self.normal = nn.BatchNorm1d(768)\n",
    "\n",
    "\n",
    "    def forward(self,conv,tuples,trg):\n",
    "        #conv = [batch_size,768]\n",
    "        #tuples = [tuples_size,768]\n",
    "        batch_size = self.hparams.batch_size\n",
    "        # seq_len = trg.shape[1]\n",
    "\n",
    "        # outputs = torch.zeros(5, batch_size , seq_len-1, self.hparams.n_vocab).to(self.hparams.device)\n",
    "        # tokens = torch.zeros(5, batch_size, seq_len-1).to(self.hparams.device)\n",
    "\n",
    "            \n",
    "        # now go next level\n",
    "        after_attn = self.attn(tuples, conv)\n",
    "        # normalize\n",
    "        conv = self.normal(conv)\n",
    "        after_attn = self.normal(after_attn)\n",
    "        trg_normal = nn.BatchNorm1d(trg.shape[1])\n",
    "        trg = trg_normal(trg)\n",
    "\n",
    "        after_decode = self.decoder(trg,conv,after_attn)\n",
    "        output = after_decode\n",
    "        # tokens[i] = self.decoder.decode(conv_emb, after_attn, seq_len) # [batch_size, seq_len]\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Hashmodel(hparams).to(device)\n",
    "# init weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.8, 0.8)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# calculate the number of trainable parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# index of <pad>\n",
    "PAD_ID = enc.encoder['<|endoftext|>']   \n",
    "# criterion\n",
    "# we ignore the loss whenever the target token is a padding token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = hparams.start_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, tuples_emb, conv_emb, trg_tokens, hparams):\n",
    "    '''\n",
    "    tuples_emb is the embedding of all the tuples\n",
    "    conv_emb is the embedding of all the conversation \n",
    "    trg_tokens is the targer response\n",
    "    '''\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    tuples_input = torch.FloatTensor(tuples_emb).to(hparams.device)\n",
    "    for epoach in range(10):\n",
    "\n",
    "        for iter in range(int(len(trg_tokens)/hparams.batch_size)):\n",
    "            batched_data = get_batched_data(trg_tokens,conv_emb, hparams.batch_size, iter)\n",
    "            conv_input = torch.FloatTensor(batched_data['conv_emb']).to(hparams.device)\n",
    "            trg_input = torch.LongTensor(batched_data['trg_tokens']).to(hparams.device)\n",
    "\n",
    "        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model.forward(conv_input, tuples_input, trg_input)\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "            # output = output[1:].view(-1, output.shape[-1])\n",
    "            # trg = trg[1:].view(-1)\n",
    "\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "\n",
    "            # print('trg:',trg_input.shape)\n",
    "            # print('output:',outputs.shape)\n",
    "            output = outputs.reshape(-1, outputs.shape[-1])\n",
    "            trg = trg_input[:,1:].reshape(-1)\n",
    "            # print('trg:',trg.shape)\n",
    "            # print('output:',output.shape)\n",
    "            # output = outputs[i]\n",
    "            # trg = trg_input[:,1:]\n",
    "            loss = criterion(output, trg) \n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"iter %d loss: %f\"%(iter,loss.item()))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "    print(\"epoach %d loss: %f\"%(epoach,epoch_loss))\n",
    "    return epoch_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbLoader(kg_path, qs_path):\n",
    "    responds = []\n",
    "    questions = []\n",
    "    r_flag = False\n",
    "    list_file = open(qs_path,'rb')\n",
    "    embeddings = pickle.load(list_file)\n",
    "    list_file.close()\n",
    "    for line in embeddings:\n",
    "        if r_flag == False:\n",
    "            questions.append(line)\n",
    "            r_flag = True\n",
    "        else:\n",
    "            responds.append(line)\n",
    "            r_flag = False\n",
    "\n",
    "    list_file = open(kg_path,'rb')\n",
    "    tuples = pickle.load(list_file)\n",
    "    list_file.close()\n",
    "    # print(len(np.array(tuples)))\n",
    "    return np.array(questions), np.array(tuples)\n",
    "    \n",
    "def TextLoader(trg_path,size):\n",
    "  with open(\"source_.txt\",'r') as fin:\n",
    "    r_flag = False\n",
    "    response_lst = []\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "      if not r_flag:\n",
    "        r_flag = True\n",
    "        continue\n",
    "      else:\n",
    "        response_lst.append(line.strip())\n",
    "        count += 1\n",
    "        if count >= size:\n",
    "          break\n",
    "        r_flag = False\n",
    "    response_token = [enc.encode(response) for response in response_lst]\n",
    "    return response_token\n",
    "\n",
    "def get_batched_data(tokens,conv_emb,batch_size,iter_num):\n",
    "  assert(len(tokens) == len(conv_emb))\n",
    "  batched_data = {}\n",
    "  st = batch_size*iter_num\n",
    "  ed = batch_size*(iter_num + 1)\n",
    "  if ed >= len(tokens):\n",
    "    ed = len(tokens)\n",
    "    \n",
    "  batched_tokens = tokens[st:ed]\n",
    "  max_len = max([len(text) for text in batched_tokens]) + 2\n",
    "  batched_pad_tokens = pad_text(batched_tokens,max_len)\n",
    "  batched_data['trg_tokens'] = batched_pad_tokens\n",
    "  batched_data['conv_emb'] = conv_emb[st:ed]\n",
    "  return batched_data\n",
    "\n",
    "def pad_text(text,max_len):\n",
    "  pad_texts = [[PAD_ID] + line + [PAD_ID]*(max_len - len(line)) for line in text]\n",
    "  return np.array(pad_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([32, 29])\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 4.00 GiB total capacity; 2.67 GiB already allocated; 2.29 MiB free; 2.91 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-84e1f25e1402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquestions_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuples_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tuples.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embeddings.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"source_.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuples_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-e963c77375dc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, tuples_emb, conv_emb, trg_tokens, hparams)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuples_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m#trg = [trg sent len, batch size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-0b28b8831365>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, conv, tuples, trg)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mafter_decode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mafter_attn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mafter_decode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# tokens[i] = self.decoder.decode(conv_emb, after_attn, seq_len) # [batch_size, seq_len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Ananonda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-310e06bb7b0c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, trg, h_c, h_k)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# outputs = torch.cat([outputs,output],dim=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[1;31m# print('outputs: ', outputs[:,1:].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 4.00 GiB total capacity; 2.67 GiB already allocated; 2.29 MiB free; 2.91 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "questions_emb, tuples_emb = EmbLoader('tuples.pickle', 'embeddings.pickle')\n",
    "tokens = TextLoader(\"source_.txt\",5000)\n",
    "train(model, criterion, optimizer, tuples_emb, questions_emb, tokens, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tue Jul 07 14:26:43 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 451.48       Driver Version: 451.48       CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1050   WDDM  | 00000000:01:00.0 Off |                  N/A |\n| N/A   63C    P8    N/A /  N/A |     75MiB /  4096MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n"
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitbasecondaf337dbccda6944288e5d672b280ddbe0",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}